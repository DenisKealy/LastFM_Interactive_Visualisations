{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################   TASK #1\n",
    "#THIS FUNCTION ALONG WITH THE sHELL SCRIPT COMBINES THE WHOLE LAST fm DATASET .THE DATASET IS HUGE AND IT IS ORGANISED IN 17576 \n",
    "# DIRECTORIES. COMBINING DATASET TAKES AROUND 2 HOURS FOR A NORMAL HARD DRIVE DUE TO MECHANICAL MOVEMENTS. WE ADVICE TO RUN \n",
    "#THIS SCRIPT WHILE KEEPING DATASET INTO SSD\n",
    "\n",
    "#PATH IS A ROOT DIRECTORY OF DATASET\n",
    "#SCRIPTPATH IS WHERE SCRIPT IS PLACED ON FILESYSTEM\n",
    "#NAME IS JUST A STRING THAT WILL SERVE AS PREFIX TO ALL COMBINED DATA\n",
    "#DESTINATIONFOLDER WILL CONTAIN COMBINED DATA\n",
    "\n",
    "def combineFiles(path,scriptPath,name,destinationFolder):\n",
    "    if(os.path.isdir(path)):\n",
    "        folders =os.listdir(path)\n",
    "        if(os.path.isdir(path+\"\\\\\"+folders[0])):\n",
    "            for dirs in folders:\n",
    "                #print(path+\"\\\\\"+dirs,name+dirs)\n",
    "                listDir(path+\"\\\\\"+dirs,scriptPath,name+dirs,destinationFolder)\n",
    "        else:\n",
    "            print(name)\n",
    "            subprocess.call([scriptPath,path,name,destinationFolder],shell=True)\n",
    "                \n",
    "#else:\n",
    "#temp.append(os.path.dirname(path))\n",
    "#subprocess.call([scriptPath,os.path.dirname(path),name,destinationFolder],shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the combined json files , there will be many files and all need to be concatenated into one single json file.\n",
    "used Cammand prompt/ terminal to concatenate these files in Destination folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended way would be to process data batchwise. If processed as one single file , memory usage will go high in latter stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################   TASK #2\n",
    "\n",
    "# PATH IS ABSOLUTE PATH OF CONCATENATED JSON FILE ON FILE SYSTEM. THIS FUNCTION RETURN A DICTIONARY \n",
    "# OBJECT WHICH IS VERY EFFICIENT WHILE WORKING WITH LARGE FILES\n",
    "def readNsplitJson(path):\n",
    "    file=open(path)\n",
    "    temp=(file.readline())\n",
    "    i=0\n",
    "    tempDict={}\n",
    "    for j in temp.split(\"}\"):\n",
    "        tempDict[i] = j+\"}\"\n",
    "        i=i+1\n",
    "    return tempDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary = readNsplitJson(r'<path>\\<filename>.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################   TASK #3\n",
    "\n",
    "#THIS FUNCTION WILL CONVERTTHE DICTIONARY INTO DATAFRAMESWHICH CAN BE UPLOADED TO hdfs OR ANY OTER DESIRABLE FILESYSTEM\n",
    "# WARNING : MEMORY USAGE WILL BE HIGH AS HUGE DATA IS BEEN READ INTO MEMORY\n",
    "\n",
    "def createNewDictionary(dictionary):\n",
    "    newDict= {}\n",
    "    k=0\n",
    "    for i in range(0,(len(dictionary)-1)):\n",
    "        #print(k,i)\n",
    "        w=json.loads(dictionary[i])['artist']\n",
    "        x=[[x,v] for x,v in json.loads(dictionary[i])[\"similars\"]]\n",
    "        y=[[x,v] for x,v in json.loads(dictionary[i])[\"tags\"]]\n",
    "        z=json.loads(dictionary[i])['track_id']\n",
    "        a=json.loads(dictionary[i])['title']\n",
    "        if(len(x)>10 and len(y)>10):\n",
    "            for j in range(0,9):\n",
    "                newDict[k]=[w,x[j][0],x[j][1],y[j][0],y[j][1],z,a]\n",
    "                k=k+1\n",
    "        else:\n",
    "            if(len(x)>len(y)):\n",
    "                for j in range(0,len(y)):\n",
    "                    newDict[k]=[w,x[j][0],x[j][1],y[j][0],y[j][1],z,a]\n",
    "                    k=k+1\n",
    "            else:\n",
    "                for j in range(0,len(x)):\n",
    "                    newDict[k]=[w,x[j][0],x[j][1],y[j][0],y[j][1],z,a]\n",
    "                    k=k+1\n",
    "                    \n",
    "    return newDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "newdict = pd.DataFrame(createNewDictionary(dictionary)).transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
